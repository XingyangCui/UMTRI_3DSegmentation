{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2beaf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: /home/cuixing/Foot&Ankle/Tarsus_split_rawdata\n",
      "nnUNet_preprocessed: /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed\n",
      "nnUNet_results: /home/cuixing/Foot&Ankle/Results\n"
     ]
    }
   ],
   "source": [
    "import nnunetv2\n",
    "#setting the path\n",
    "\n",
    "import os\n",
    "os.environ[\"nnUNet_raw\"] = \"/home/cuixing/Foot&Ankle/Tarsus_split_rawdata\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/home/cuixing/Foot&Ankle/Results\"  # 训练结果存放目录\n",
    "# =Check\n",
    "print(\"nnUNet_raw:\", os.environ.get(\"nnUNet_raw\"))\n",
    "print(\"nnUNet_preprocessed:\", os.environ.get(\"nnUNet_preprocessed\"))\n",
    "print(\"nnUNet_results:\", os.environ.get(\"nnUNet_results\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5948ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-07-06 18:46:06.703619: Using torch.compile...\n",
      "/home/cuixing/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/load_pretrained_weights.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_model = torch.load(fname)\n",
      "################### Loading pretrained weights from file  /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/TS_FineTuning/nnUNet_results/Dataset072_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth ###################\n",
      "Below is the list of overlapping blocks in pretrained model and nnUNet architecture:\n",
      "encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])\n",
      "decoder.stages.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])\n",
      "decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.stages.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])\n",
      "decoder.stages.1.convs.0.conv.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.norm.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.norm.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])\n",
      "decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.stages.1.convs.1.conv.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.norm.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.norm.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])\n",
      "decoder.stages.2.convs.0.conv.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.norm.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.norm.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])\n",
      "decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.stages.2.convs.1.conv.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.norm.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.norm.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])\n",
      "decoder.stages.3.convs.0.conv.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.norm.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.norm.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])\n",
      "decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.stages.3.convs.1.conv.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.norm.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.norm.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])\n",
      "decoder.stages.4.convs.0.conv.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.norm.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.norm.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])\n",
      "decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.stages.4.convs.1.conv.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.norm.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.norm.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.transpconvs.0.weight shape torch.Size([320, 320, 2, 1, 1])\n",
      "decoder.transpconvs.0.bias shape torch.Size([320])\n",
      "decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])\n",
      "decoder.transpconvs.1.bias shape torch.Size([256])\n",
      "decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])\n",
      "decoder.transpconvs.2.bias shape torch.Size([128])\n",
      "decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])\n",
      "decoder.transpconvs.3.bias shape torch.Size([64])\n",
      "decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])\n",
      "decoder.transpconvs.4.bias shape torch.Size([32])\n",
      "################### Done ###################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-06 18:46:08.634090: do_dummy_2d_data_aug: False\n",
      "2025-07-06 18:46:08.663750: Using splits from existing split file: /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones/splits_final.json\n",
      "2025-07-06 18:46:08.666704: The split file contains 1 splits.\n",
      "2025-07-06 18:46:08.667614: Desired fold for training: 0\n",
      "2025-07-06 18:46:08.668547: This split has 8 training and 4 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [256, 96, 80], 'median_image_size_in_voxels': [1728.5, 533.0, 533.0], 'spacing': [0.75, 0.75, 0.75], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_TS_app_bones', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.75, 0.75, 0.75], 'original_median_shape_after_transp': [1728, 533, 533], 'image_reader_writer': 'NibabelIOWithReorient', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2265.0, 'mean': 696.6413807913696, 'median': 483.0, 'min': -266.0, 'percentile_00_5': 79.0, 'percentile_99_5': 2123.0, 'std': 545.0568060794525}}} \n",
      "\n",
      "2025-07-06 18:47:12.316342: unpacking dataset...\n",
      "2025-07-06 18:48:16.769172: unpacking done...\n",
      "2025-07-06 18:48:16.790866: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-07-06 18:48:16.857654: \n",
      "2025-07-06 18:48:16.858793: Epoch 0\n",
      "2025-07-06 18:48:16.859905: Current learning rate: 0.01\n",
      "2025-07-06 18:50:58.826833: train_loss 0.2056\n",
      "2025-07-06 18:50:58.829636: val_loss -0.0506\n",
      "2025-07-06 18:50:58.830946: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:50:58.832067: Epoch time: 161.97 s\n",
      "2025-07-06 18:50:58.833099: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2025-07-06 18:51:01.554854: \n",
      "2025-07-06 18:51:01.556035: Epoch 1\n",
      "2025-07-06 18:51:01.557065: Current learning rate: 0.00999\n",
      "2025-07-06 18:52:00.977071: train_loss 0.0477\n",
      "2025-07-06 18:52:00.979310: val_loss 0.0996\n",
      "2025-07-06 18:52:00.980584: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0108]\n",
      "2025-07-06 18:52:00.981739: Epoch time: 59.42 s\n",
      "2025-07-06 18:52:00.982706: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2025-07-06 18:52:03.966299: \n",
      "2025-07-06 18:52:03.967574: Epoch 2\n",
      "2025-07-06 18:52:03.968597: Current learning rate: 0.00998\n",
      "2025-07-06 18:53:03.689208: train_loss 0.0861\n",
      "2025-07-06 18:53:03.694386: val_loss 0.0299\n",
      "2025-07-06 18:53:03.695638: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:53:03.696649: Epoch time: 59.72 s\n",
      "2025-07-06 18:53:05.534353: \n",
      "2025-07-06 18:53:05.535733: Epoch 3\n",
      "2025-07-06 18:53:05.536776: Current learning rate: 0.00997\n",
      "2025-07-06 18:54:05.100383: train_loss 0.086\n",
      "2025-07-06 18:54:05.106205: val_loss 0.0336\n",
      "2025-07-06 18:54:05.107437: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:54:05.108492: Epoch time: 59.57 s\n",
      "2025-07-06 18:54:06.810462: \n",
      "2025-07-06 18:54:06.811847: Epoch 4\n",
      "2025-07-06 18:54:06.812992: Current learning rate: 0.00996\n",
      "2025-07-06 18:55:06.609328: train_loss 0.0042\n",
      "2025-07-06 18:55:06.614234: val_loss -0.0548\n",
      "2025-07-06 18:55:06.615458: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:55:06.616544: Epoch time: 59.8 s\n",
      "2025-07-06 18:55:08.360357: \n",
      "2025-07-06 18:55:08.361575: Epoch 5\n",
      "2025-07-06 18:55:08.362577: Current learning rate: 0.00995\n",
      "2025-07-06 18:56:08.737637: train_loss -0.0065\n",
      "2025-07-06 18:56:08.741565: val_loss -0.0886\n",
      "2025-07-06 18:56:08.742930: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:56:08.743952: Epoch time: 60.38 s\n",
      "2025-07-06 18:56:10.448894: \n",
      "2025-07-06 18:56:10.450162: Epoch 6\n",
      "2025-07-06 18:56:10.451365: Current learning rate: 0.00995\n",
      "2025-07-06 18:57:10.071750: train_loss 0.0175\n",
      "2025-07-06 18:57:10.076243: val_loss 0.0113\n",
      "2025-07-06 18:57:10.077490: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:57:10.078648: Epoch time: 59.62 s\n",
      "2025-07-06 18:57:11.719252: \n",
      "2025-07-06 18:57:11.720553: Epoch 7\n",
      "2025-07-06 18:57:11.721632: Current learning rate: 0.00994\n"
     ]
    }
   ],
   "source": [
    "#Method 1: directly training\n",
    "\n",
    "!nnUNetv2_train 001 3d_fullres 0 \\\n",
    "  -tr nnUNetTrainer \\\n",
    "  -p nnUNetPlans \\\n",
    " -pretrained_weights /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/TS_FineTuning/nnUNet_results/Dataset072_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de166a3",
   "metadata": {},
   "source": [
    "# Method 2:Frozen encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52bc0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 plans.json 中 3d_fullres 配置的所有字段：\n",
      "\n",
      "🔹 data_identifier: nnUNetPlans_3d_fullres\n",
      "🔹 preprocessor_name: DefaultPreprocessor\n",
      "🔹 batch_size: 2\n",
      "🔹 patch_size: [256, 96, 80]\n",
      "🔹 median_image_size_in_voxels: [1728.5, 533.0, 533.0]\n",
      "🔹 spacing: [0.75, 0.75, 0.75]\n",
      "🔹 normalization_schemes: ['CTNormalization']\n",
      "🔹 use_mask_for_norm: [False]\n",
      "🔹 resampling_fn_data: resample_data_or_seg_to_shape\n",
      "🔹 resampling_fn_seg: resample_data_or_seg_to_shape\n",
      "🔹 resampling_fn_data_kwargs: {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}\n",
      "🔹 resampling_fn_seg_kwargs: {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}\n",
      "🔹 resampling_fn_probabilities: resample_data_or_seg_to_shape\n",
      "🔹 resampling_fn_probabilities_kwargs: {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}\n",
      "🔹 architecture: {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}\n",
      "🔹 batch_dice: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "plans_path = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\"\n",
    "\n",
    "with open(plans_path, 'r') as f:\n",
    "    plans = json.load(f)\n",
    "\n",
    "cfg = plans[\"configurations\"][\"3d_fullres\"]\n",
    "\n",
    "print(\"📊 plans.json 中 3d_fullres 配置的所有字段：\\n\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"🔹 {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "640299dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 基本网络结构信息：\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'spacing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4001005/3992943024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔹 图像 spacing: {plans['spacing']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔹 批大小 (batch_size): {plans['batch_size']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🔹 使用 batch dice: {plans.get('batch_dice', 'N/A')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'spacing'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "021232c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4001005/3543766363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 初始化并加载\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplans_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3d_fullres\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4001005/3543766363.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plans, configuration, fold, dataset_json)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_dataset_folder_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 初始化并加载\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plans, configuration, fold, dataset_json, unpack_dataset, device)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                        self.__class__.__name__ + '__' + self.plans_manager.plans_name + \"__\" + configuration) \\\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnnUNet_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'fold_{fold}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         self.preprocessed_dataset_folder = join(self.preprocessed_dataset_folder_base,\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer\n",
    "\n",
    "# 配置路径\n",
    "ckpt_path = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_best.pth\"\n",
    "plans_path = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\"\n",
    "dataset_json_path = \"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones/dataset.json\"\n",
    "\n",
    "# 读取 JSON\n",
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "# ✅ 子类包装，提前赋值路径\n",
    "class MyTrainer(nnUNetTrainer):\n",
    "    def __init__(self, plans, configuration, fold, dataset_json):\n",
    "        # 提前赋值路径（这些变量 super() 里要用）\n",
    "        self.output_folder_base = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones\"\n",
    "        self.preprocessed_dataset_folder_base = \"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones\"\n",
    "        super().__init__(plans, configuration, fold, dataset_json)\n",
    "\n",
    "# 初始化并加载\n",
    "trainer = MyTrainer(plans_path, \"3d_fullres\", 0, dataset_json)\n",
    "trainer.initialize()\n",
    "\n",
    "# 加载权重\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "trainer.network.load_state_dict(ckpt[\"network_weights\"])\n",
    "\n",
    "# 打印网络结构\n",
    "print(\"📐 Network structure:\\n\")\n",
    "print(trainer.network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb2b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cuixing/.local/bin/nnUNetv2_train\", line 8, in <module>\n",
      "    sys.exit(run_training_entry())\n",
      "  File \"/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/run_training.py\", line 275, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/run_training.py\", line 196, in run_training\n",
      "    nnunet_trainer = get_trainer_from_args(dataset_name_or_id, configuration, fold, trainer_class_name,\n",
      "  File \"/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/run_training.py\", line 43, in get_trainer_from_args\n",
      "    raise RuntimeError(f'Could not find requested nnunet trainer {trainer_name} in '\n",
      "RuntimeError: Could not find requested nnunet trainer FrozenEncoderTrainer in nnunetv2.training.nnUNetTrainer (/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/training/nnUNetTrainer). If it is located somewhere else, please move it there.\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train 001 3d_fullres 2 \\\n",
    "-tr FrozenEncoderTrainer \\\n",
    "-pretrained_weights /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/TS_FineTuning/nnUNet_results/Dataset072_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c062c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
