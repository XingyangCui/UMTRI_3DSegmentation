{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2beaf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw: /home/cuixing/Foot&Ankle/Tarsus_split_rawdata\n",
      "nnUNet_preprocessed: /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed\n",
      "nnUNet_results: /home/cuixing/Foot&Ankle/Results\n"
     ]
    }
   ],
   "source": [
    "import nnunetv2\n",
    "#setting the path\n",
    "\n",
    "import os\n",
    "os.environ[\"nnUNet_raw\"] = \"/home/cuixing/Foot&Ankle/Tarsus_split_rawdata\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/home/cuixing/Foot&Ankle/Results\"  # ËÆ≠ÁªÉÁªìÊûúÂ≠òÊîæÁõÆÂΩï\n",
    "# =Check\n",
    "print(\"nnUNet_raw:\", os.environ.get(\"nnUNet_raw\"))\n",
    "print(\"nnUNet_preprocessed:\", os.environ.get(\"nnUNet_preprocessed\"))\n",
    "print(\"nnUNet_results:\", os.environ.get(\"nnUNet_results\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5948ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2025-07-06 18:46:06.703619: Using torch.compile...\n",
      "/home/cuixing/.local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/load_pretrained_weights.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  saved_model = torch.load(fname)\n",
      "################### Loading pretrained weights from file  /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/TS_FineTuning/nnUNet_results/Dataset072_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth ###################\n",
      "Below is the list of overlapping blocks in pretrained model and nnUNet architecture:\n",
      "encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.0.0.convs.0.conv.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.0.conv.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.norm.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.norm.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.0.weight shape torch.Size([32, 1, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.1.conv.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.norm.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.norm.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.encoder.stages.0.0.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.encoder.stages.1.0.convs.0.conv.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.0.conv.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.norm.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.norm.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.0.weight shape torch.Size([64, 32, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.1.conv.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.norm.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.norm.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.encoder.stages.1.0.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.encoder.stages.2.0.convs.0.conv.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.0.conv.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.norm.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.norm.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.0.weight shape torch.Size([128, 64, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.1.conv.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.norm.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.norm.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.encoder.stages.2.0.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.encoder.stages.3.0.convs.0.conv.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.0.conv.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.norm.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.norm.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.0.weight shape torch.Size([256, 128, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.1.conv.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.norm.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.norm.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.encoder.stages.3.0.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.encoder.stages.4.0.convs.0.conv.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.0.weight shape torch.Size([320, 256, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.4.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.encoder.stages.5.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.conv.weight shape torch.Size([320, 640, 3, 3, 3])\n",
      "decoder.stages.0.convs.0.conv.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.norm.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.norm.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.0.weight shape torch.Size([320, 640, 3, 3, 3])\n",
      "decoder.stages.0.convs.0.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.0.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.conv.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.stages.0.convs.1.conv.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.norm.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.norm.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.0.weight shape torch.Size([320, 320, 3, 3, 3])\n",
      "decoder.stages.0.convs.1.all_modules.0.bias shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.1.weight shape torch.Size([320])\n",
      "decoder.stages.0.convs.1.all_modules.1.bias shape torch.Size([320])\n",
      "decoder.stages.1.convs.0.conv.weight shape torch.Size([256, 512, 3, 3, 3])\n",
      "decoder.stages.1.convs.0.conv.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.norm.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.norm.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.0.weight shape torch.Size([256, 512, 3, 3, 3])\n",
      "decoder.stages.1.convs.0.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.0.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.conv.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.stages.1.convs.1.conv.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.norm.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.norm.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.0.weight shape torch.Size([256, 256, 3, 3, 3])\n",
      "decoder.stages.1.convs.1.all_modules.0.bias shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.1.weight shape torch.Size([256])\n",
      "decoder.stages.1.convs.1.all_modules.1.bias shape torch.Size([256])\n",
      "decoder.stages.2.convs.0.conv.weight shape torch.Size([128, 256, 3, 3, 3])\n",
      "decoder.stages.2.convs.0.conv.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.norm.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.norm.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.0.weight shape torch.Size([128, 256, 3, 3, 3])\n",
      "decoder.stages.2.convs.0.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.0.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.conv.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.stages.2.convs.1.conv.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.norm.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.norm.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.0.weight shape torch.Size([128, 128, 3, 3, 3])\n",
      "decoder.stages.2.convs.1.all_modules.0.bias shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.1.weight shape torch.Size([128])\n",
      "decoder.stages.2.convs.1.all_modules.1.bias shape torch.Size([128])\n",
      "decoder.stages.3.convs.0.conv.weight shape torch.Size([64, 128, 3, 3, 3])\n",
      "decoder.stages.3.convs.0.conv.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.norm.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.norm.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.0.weight shape torch.Size([64, 128, 3, 3, 3])\n",
      "decoder.stages.3.convs.0.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.0.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.conv.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.stages.3.convs.1.conv.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.norm.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.norm.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.0.weight shape torch.Size([64, 64, 3, 3, 3])\n",
      "decoder.stages.3.convs.1.all_modules.0.bias shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.1.weight shape torch.Size([64])\n",
      "decoder.stages.3.convs.1.all_modules.1.bias shape torch.Size([64])\n",
      "decoder.stages.4.convs.0.conv.weight shape torch.Size([32, 64, 3, 3, 3])\n",
      "decoder.stages.4.convs.0.conv.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.norm.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.norm.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.0.weight shape torch.Size([32, 64, 3, 3, 3])\n",
      "decoder.stages.4.convs.0.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.0.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.conv.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.stages.4.convs.1.conv.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.norm.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.norm.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.0.weight shape torch.Size([32, 32, 3, 3, 3])\n",
      "decoder.stages.4.convs.1.all_modules.0.bias shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.1.weight shape torch.Size([32])\n",
      "decoder.stages.4.convs.1.all_modules.1.bias shape torch.Size([32])\n",
      "decoder.transpconvs.0.weight shape torch.Size([320, 320, 2, 1, 1])\n",
      "decoder.transpconvs.0.bias shape torch.Size([320])\n",
      "decoder.transpconvs.1.weight shape torch.Size([320, 256, 2, 2, 2])\n",
      "decoder.transpconvs.1.bias shape torch.Size([256])\n",
      "decoder.transpconvs.2.weight shape torch.Size([256, 128, 2, 2, 2])\n",
      "decoder.transpconvs.2.bias shape torch.Size([128])\n",
      "decoder.transpconvs.3.weight shape torch.Size([128, 64, 2, 2, 2])\n",
      "decoder.transpconvs.3.bias shape torch.Size([64])\n",
      "decoder.transpconvs.4.weight shape torch.Size([64, 32, 2, 2, 2])\n",
      "decoder.transpconvs.4.bias shape torch.Size([32])\n",
      "################### Done ###################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-06 18:46:08.634090: do_dummy_2d_data_aug: False\n",
      "2025-07-06 18:46:08.663750: Using splits from existing split file: /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones/splits_final.json\n",
      "2025-07-06 18:46:08.666704: The split file contains 1 splits.\n",
      "2025-07-06 18:46:08.667614: Desired fold for training: 0\n",
      "2025-07-06 18:46:08.668547: This split has 8 training and 4 validation cases.\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 3d_fullres\n",
      " {'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': [256, 96, 80], 'median_image_size_in_voxels': [1728.5, 533.0, 533.0], 'spacing': [0.75, 0.75, 0.75], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_TS_app_bones', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [0.75, 0.75, 0.75], 'original_median_shape_after_transp': [1728, 533, 533], 'image_reader_writer': 'NibabelIOWithReorient', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 2265.0, 'mean': 696.6413807913696, 'median': 483.0, 'min': -266.0, 'percentile_00_5': 79.0, 'percentile_99_5': 2123.0, 'std': 545.0568060794525}}} \n",
      "\n",
      "2025-07-06 18:47:12.316342: unpacking dataset...\n",
      "2025-07-06 18:48:16.769172: unpacking done...\n",
      "2025-07-06 18:48:16.790866: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2025-07-06 18:48:16.857654: \n",
      "2025-07-06 18:48:16.858793: Epoch 0\n",
      "2025-07-06 18:48:16.859905: Current learning rate: 0.01\n",
      "2025-07-06 18:50:58.826833: train_loss 0.2056\n",
      "2025-07-06 18:50:58.829636: val_loss -0.0506\n",
      "2025-07-06 18:50:58.830946: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:50:58.832067: Epoch time: 161.97 s\n",
      "2025-07-06 18:50:58.833099: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2025-07-06 18:51:01.554854: \n",
      "2025-07-06 18:51:01.556035: Epoch 1\n",
      "2025-07-06 18:51:01.557065: Current learning rate: 0.00999\n",
      "2025-07-06 18:52:00.977071: train_loss 0.0477\n",
      "2025-07-06 18:52:00.979310: val_loss 0.0996\n",
      "2025-07-06 18:52:00.980584: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0108]\n",
      "2025-07-06 18:52:00.981739: Epoch time: 59.42 s\n",
      "2025-07-06 18:52:00.982706: Yayy! New best EMA pseudo Dice: 0.0\n",
      "2025-07-06 18:52:03.966299: \n",
      "2025-07-06 18:52:03.967574: Epoch 2\n",
      "2025-07-06 18:52:03.968597: Current learning rate: 0.00998\n",
      "2025-07-06 18:53:03.689208: train_loss 0.0861\n",
      "2025-07-06 18:53:03.694386: val_loss 0.0299\n",
      "2025-07-06 18:53:03.695638: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:53:03.696649: Epoch time: 59.72 s\n",
      "2025-07-06 18:53:05.534353: \n",
      "2025-07-06 18:53:05.535733: Epoch 3\n",
      "2025-07-06 18:53:05.536776: Current learning rate: 0.00997\n",
      "2025-07-06 18:54:05.100383: train_loss 0.086\n",
      "2025-07-06 18:54:05.106205: val_loss 0.0336\n",
      "2025-07-06 18:54:05.107437: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:54:05.108492: Epoch time: 59.57 s\n",
      "2025-07-06 18:54:06.810462: \n",
      "2025-07-06 18:54:06.811847: Epoch 4\n",
      "2025-07-06 18:54:06.812992: Current learning rate: 0.00996\n",
      "2025-07-06 18:55:06.609328: train_loss 0.0042\n",
      "2025-07-06 18:55:06.614234: val_loss -0.0548\n",
      "2025-07-06 18:55:06.615458: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:55:06.616544: Epoch time: 59.8 s\n",
      "2025-07-06 18:55:08.360357: \n",
      "2025-07-06 18:55:08.361575: Epoch 5\n",
      "2025-07-06 18:55:08.362577: Current learning rate: 0.00995\n",
      "2025-07-06 18:56:08.737637: train_loss -0.0065\n",
      "2025-07-06 18:56:08.741565: val_loss -0.0886\n",
      "2025-07-06 18:56:08.742930: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:56:08.743952: Epoch time: 60.38 s\n",
      "2025-07-06 18:56:10.448894: \n",
      "2025-07-06 18:56:10.450162: Epoch 6\n",
      "2025-07-06 18:56:10.451365: Current learning rate: 0.00995\n",
      "2025-07-06 18:57:10.071750: train_loss 0.0175\n",
      "2025-07-06 18:57:10.076243: val_loss 0.0113\n",
      "2025-07-06 18:57:10.077490: Pseudo dice [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "2025-07-06 18:57:10.078648: Epoch time: 59.62 s\n",
      "2025-07-06 18:57:11.719252: \n",
      "2025-07-06 18:57:11.720553: Epoch 7\n",
      "2025-07-06 18:57:11.721632: Current learning rate: 0.00994\n"
     ]
    }
   ],
   "source": [
    "#Method 1: directly training\n",
    "\n",
    "!nnUNetv2_train 001 3d_fullres 0 \\\n",
    "  -tr nnUNetTrainer \\\n",
    "  -p nnUNetPlans \\\n",
    " -pretrained_weights /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/TS_FineTuning/nnUNet_results/Dataset072_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de166a3",
   "metadata": {},
   "source": [
    "# Method 2:Frozen encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52bc0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä plans.json ‰∏≠ 3d_fullres ÈÖçÁΩÆÁöÑÊâÄÊúâÂ≠óÊÆµÔºö\n",
      "\n",
      "üîπ data_identifier: nnUNetPlans_3d_fullres\n",
      "üîπ preprocessor_name: DefaultPreprocessor\n",
      "üîπ batch_size: 2\n",
      "üîπ patch_size: [256, 96, 80]\n",
      "üîπ median_image_size_in_voxels: [1728.5, 533.0, 533.0]\n",
      "üîπ spacing: [0.75, 0.75, 0.75]\n",
      "üîπ normalization_schemes: ['CTNormalization']\n",
      "üîπ use_mask_for_norm: [False]\n",
      "üîπ resampling_fn_data: resample_data_or_seg_to_shape\n",
      "üîπ resampling_fn_seg: resample_data_or_seg_to_shape\n",
      "üîπ resampling_fn_data_kwargs: {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}\n",
      "üîπ resampling_fn_seg_kwargs: {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}\n",
      "üîπ resampling_fn_probabilities: resample_data_or_seg_to_shape\n",
      "üîπ resampling_fn_probabilities_kwargs: {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}\n",
      "üîπ architecture: {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 1, 1]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}\n",
      "üîπ batch_dice: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "plans_path = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\"\n",
    "\n",
    "with open(plans_path, 'r') as f:\n",
    "    plans = json.load(f)\n",
    "\n",
    "cfg = plans[\"configurations\"][\"3d_fullres\"]\n",
    "\n",
    "print(\"üìä plans.json ‰∏≠ 3d_fullres ÈÖçÁΩÆÁöÑÊâÄÊúâÂ≠óÊÆµÔºö\\n\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"üîπ {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "640299dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Âü∫Êú¨ÁΩëÁªúÁªìÊûÑ‰ø°ÊÅØÔºö\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'spacing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4001005/3992943024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üîπ ÂõæÂÉè spacing: {plans['spacing']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üîπ ÊâπÂ§ßÂ∞è (batch_size): {plans['batch_size']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"üîπ ‰ΩøÁî® batch dice: {plans.get('batch_dice', 'N/A')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'spacing'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "021232c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4001005/3543766363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ÂàùÂßãÂåñÂπ∂Âä†ËΩΩ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplans_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3d_fullres\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4001005/3543766363.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plans, configuration, fold, dataset_json)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_dataset_folder_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# ÂàùÂßãÂåñÂπ∂Âä†ËΩΩ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plans, configuration, fold, dataset_json, unpack_dataset, device)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                        self.__class__.__name__ + '__' + self.plans_manager.plans_name + \"__\" + configuration) \\\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnnUNet_results\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_folder_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'fold_{fold}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         self.preprocessed_dataset_folder = join(self.preprocessed_dataset_folder_base,\n",
      "\u001b[0;32m/sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer\n",
    "\n",
    "# ÈÖçÁΩÆË∑ØÂæÑ\n",
    "ckpt_path = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_0/checkpoint_best.pth\"\n",
    "plans_path = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/plans.json\"\n",
    "dataset_json_path = \"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones/dataset.json\"\n",
    "\n",
    "# ËØªÂèñ JSON\n",
    "with open(dataset_json_path, 'r') as f:\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "# ‚úÖ Â≠êÁ±ªÂåÖË£ÖÔºåÊèêÂâçËµãÂÄºË∑ØÂæÑ\n",
    "class MyTrainer(nnUNetTrainer):\n",
    "    def __init__(self, plans, configuration, fold, dataset_json):\n",
    "        # ÊèêÂâçËµãÂÄºË∑ØÂæÑÔºàËøô‰∫õÂèòÈáè super() ÈáåË¶ÅÁî®Ôºâ\n",
    "        self.output_folder_base = \"/home/cuixing/Foot&Ankle/Results/Dataset001_TS_app_bones\"\n",
    "        self.preprocessed_dataset_folder_base = \"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/Xingyang_Cui/Data_Preprocessed/Dataset001_TS_app_bones\"\n",
    "        super().__init__(plans, configuration, fold, dataset_json)\n",
    "\n",
    "# ÂàùÂßãÂåñÂπ∂Âä†ËΩΩ\n",
    "trainer = MyTrainer(plans_path, \"3d_fullres\", 0, dataset_json)\n",
    "trainer.initialize()\n",
    "\n",
    "# Âä†ËΩΩÊùÉÈáç\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "trainer.network.load_state_dict(ckpt[\"network_weights\"])\n",
    "\n",
    "# ÊâìÂç∞ÁΩëÁªúÁªìÊûÑ\n",
    "print(\"üìê Network structure:\\n\")\n",
    "print(trainer.network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb2b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cuixing/.local/bin/nnUNetv2_train\", line 8, in <module>\n",
      "    sys.exit(run_training_entry())\n",
      "  File \"/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/run_training.py\", line 275, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/run_training.py\", line 196, in run_training\n",
      "    nnunet_trainer = get_trainer_from_args(dataset_name_or_id, configuration, fold, trainer_class_name,\n",
      "  File \"/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/run/run_training.py\", line 43, in get_trainer_from_args\n",
      "    raise RuntimeError(f'Could not find requested nnunet trainer {trainer_name} in '\n",
      "RuntimeError: Could not find requested nnunet trainer FrozenEncoderTrainer in nnunetv2.training.nnUNetTrainer (/home/cuixing/.local/lib/python3.9/site-packages/nnunetv2/training/nnUNetTrainer). If it is located somewhere else, please move it there.\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_train 001 3d_fullres 2 \\\n",
    "-tr FrozenEncoderTrainer \\\n",
    "-pretrained_weights /nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/TS_FineTuning/nnUNet_results/Dataset072_TS_app_bones/nnUNetTrainer__nnUNetPlans__3d_fullres/fold_1/checkpoint_best.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c062c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
