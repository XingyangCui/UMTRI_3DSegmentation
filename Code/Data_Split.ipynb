{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65259c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2794961134.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3722430/2794961134.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print(f\"{i+1:3}: {line}\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/cuixing/.local/lib/python3.9/site-packages/totalsegmentator/map_to_binary.py\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "    for i, line in enumerate(content.splitlines()):\n",
    "    print(f\"{i+1:3}: {line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffadf382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    \"background\": 0,\n",
    "    \"Femur\": 1,\n",
    "    \"Fibula\": 2,\n",
    "    \"T1_S1\": 3,\n",
    "    \"T1_S3\": 4,\n",
    "    \"T1_S4\": 5,\n",
    "    \"T2_S1\": 6,\n",
    "    \"T2_S2\": 7,\n",
    "    \"T2_S3\": 8,\n",
    "    \"T2_S4\": 9,\n",
    "    \"T3_S1\": 10,\n",
    "    \"T3_S2\": 11,\n",
    "    \"T3_S3\": 12,\n",
    "    \"T3_S4\": 13,\n",
    "    \"T4_S1\": 14,\n",
    "    \"T4_S2\": 15,\n",
    "    \"T4_S3\": 16,\n",
    "    \"T4_S4\": 17,\n",
    "    \"T5_S1\": 18,\n",
    "    \"T5_S2\": 19,\n",
    "    \"T5_S3\": 20,\n",
    "    \"T5_S4\": 21,\n",
    "    \"Calcaneus\": 22,\n",
    "    \"Cuboid\": 23,\n",
    "    \"Cunei_Med\": 24,\n",
    "    \"Cunei_Lat\": 25,\n",
    "    \"Cunei_Int\": 26,\n",
    "    \"Navicular\": 27,\n",
    "    \"Talus\": 28,\n",
    "    \"Tibia\": 29\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d2e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Processing training+validation set in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "# é…ç½®è·¯å¾„\n",
    "os.environ['nnUNet_raw'] = '/home/cuixing/Foot&Ankle/Tarsus_split_rawdata'\n",
    "os.environ['nnUNet_preprocessed'] = '/home/cuixing/Foot&Ankle/Data_Preprocessed'\n",
    "\n",
    "\n",
    "\n",
    "def combine_labels(ref_img, file_out, class_map, seg_dir):\n",
    "    ref_img = nib.load(ref_img)\n",
    "    combined = np.zeros(ref_img.shape, dtype=np.uint8)\n",
    "    missing = []\n",
    "    multiple = []\n",
    "\n",
    "    for label_name, label_id in class_map.items():\n",
    "        if label_name == \"background\":\n",
    "            continue\n",
    "        matched = list(seg_dir.glob(f\"*{label_name}*.nii.gz\"))\n",
    "        if not matched:\n",
    "            missing.append(label_name)\n",
    "            continue\n",
    "        if len(matched) > 1:\n",
    "            multiple.append(label_name)\n",
    "        img = nib.load(matched[0])\n",
    "        combined[img.get_fdata() > 0] = label_id\n",
    "\n",
    "    nib.save(nib.Nifti1Image(combined, ref_img.affine), file_out)\n",
    "    return missing\n",
    "\n",
    "\n",
    "def process_train(subject):\n",
    "    subject_path = dataset_path / subject\n",
    "    ct_file = subject_path / f\"{subject}_resampled.nii.gz\"\n",
    "    seg_dir = subject_path / f\"{subject}_GT_Segmentations\"\n",
    "    if not ct_file.exists():\n",
    "        return subject, \"missing\", []\n",
    "\n",
    "    shutil.copy(ct_file, nnunet_path / \"imagesTr\" / f\"{subject}_0000.nii.gz\")\n",
    "    label_out = nnunet_path / \"labelsTr\" / f\"{subject}.nii.gz\"\n",
    "    missing = combine_labels(ct_file, label_out, class_map, seg_dir)\n",
    "    return subject, \"ok\", missing\n",
    "\n",
    "\n",
    "def process_test(subject):\n",
    "    subject_path = dataset_path / subject\n",
    "    ct_file = subject_path / f\"{subject}_resampled.nii.gz\"\n",
    "    seg_dir = subject_path / f\"{subject}_GT_Segmentations\"\n",
    "    if not ct_file.exists():\n",
    "        return subject, \"missing\", []\n",
    "\n",
    "    shutil.copy(ct_file, nnunet_path / \"imagesTs\" / f\"{subject}_0000.nii.gz\")\n",
    "    label_out = nnunet_path / \"labelsTs\" / f\"{subject}.nii.gz\"\n",
    "    missing = combine_labels(ct_file, label_out, class_map, seg_dir)\n",
    "    return subject, \"ok\", missing\n",
    "\n",
    "\n",
    "def generate_json_from_dir_v2(foldername, subjects_train, subjects_val, labels):\n",
    "    print(\"ğŸ“„ Creating dataset.json...\")\n",
    "    out_base = Path(os.environ['nnUNet_raw']) / foldername\n",
    "    json_dict = {\n",
    "        \"name\": \"TotalSegmentator\",\n",
    "        \"description\": \"Segmentation of TotalSegmentator classes\",\n",
    "        \"reference\": \"https://zenodo.org/record/6802614\",\n",
    "        \"licence\": \"Apache 2.0\",\n",
    "        \"release\": \"2.0\",\n",
    "        \"channel_names\": {\"0\": \"CT\"},\n",
    "        \"labels\": class_map,\n",
    "        \"numTraining\": len(subjects_train + subjects_val),\n",
    "        \"file_ending\": \".nii.gz\",\n",
    "        \"overwrite_image_reader_writer\": \"NibabelIOWithReorient\",\n",
    "        \"training\": [\n",
    "            {\n",
    "                \"image\": f\"./imagesTr/{subj}_0000.nii.gz\",\n",
    "                \"label\": f\"./labelsTr/{subj}.nii.gz\"\n",
    "            }\n",
    "            for subj in subjects_train + subjects_val\n",
    "        ]\n",
    "    }\n",
    "    json.dump(json_dict, open(out_base / \"dataset.json\", \"w\"), indent=4)\n",
    "\n",
    "    print(\"ğŸ“„ Creating splits_final.json...\")\n",
    "    output_folder_pkl = Path(os.environ['nnUNet_preprocessed']) / foldername\n",
    "    output_folder_pkl.mkdir(parents=True, exist_ok=True)\n",
    "    splits = [{\"train\": subjects_train, \"val\": subjects_val}]\n",
    "    json.dump(splits, open(output_folder_pkl / \"splits_final.json\", \"w\"), indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = Path(\"/nfs/turbo/coe-mreedsensitive/Processing/Foot_and_Ankle/SK/Tarsus_Separated\")\n",
    "    nnunet_path = Path(\"/home/cuixing/Foot&Ankle/Tarsus_split_rawdata/Dataset001_TS_app_bones\")\n",
    "\n",
    "    for subdir in [\"imagesTr\", \"labelsTr\", \"imagesTs\", \"labelsTs\"]:\n",
    "        (nnunet_path / subdir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    meta = pd.read_csv(dataset_path / \"meta.csv\", sep=\",\")\n",
    "    subjects_train = list(meta[meta[\"split\"] == \"train\"][\"image_id\"].values)\n",
    "    subjects_val = list(meta[meta[\"split\"] == \"val\"][\"image_id\"].values)\n",
    "    subjects_test = list(meta[meta[\"split\"] == \"test\"][\"image_id\"].values)\n",
    "\n",
    "    print(\"ğŸ“¦ Processing training+validation set in parallel...\")\n",
    "    missing_counts = {}\n",
    "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        for subject, status, missing in tqdm(executor.map(process_train, subjects_train + subjects_val), total=len(subjects_train + subjects_val)):\n",
    "            if status == \"missing\":\n",
    "                print(f\"âŒ Missing CT for {subject}\")\n",
    "            for m in missing:\n",
    "                missing_counts[m] = missing_counts.get(m, 0) + 1\n",
    "\n",
    "    print(\"ğŸ“¦ Processing test set in parallel...\")\n",
    "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        for subject, status, missing in tqdm(executor.map(process_test, subjects_test), total=len(subjects_test)):\n",
    "            if status == \"missing\":\n",
    "                print(f\"âŒ Missing CT for {subject}\")\n",
    "            for m in missing:\n",
    "                missing_counts[m] = missing_counts.get(m, 0) + 1\n",
    "\n",
    "    generate_json_from_dir_v2(nnunet_path.name, subjects_train, subjects_val, class_map.keys())\n",
    "\n",
    "    print(\"\\nğŸ“Š ç¼ºå¤±ç»“æ„ç»Ÿè®¡æŠ¥å‘Š:\")\n",
    "    for label, count in sorted(missing_counts.items(), key=lambda x: -x[1]):\n",
    "        print(f\" - {label:<15}: missing in {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7707bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
